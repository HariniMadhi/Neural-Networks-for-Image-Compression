{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Image Compression Project MLP part\n",
    "\n",
    "This code applies predictive coding algoritm for lossless image compression with a basic MLP structure. Details of predictive coding algorithm can be found [here](https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "The code has four parts\n",
    "\n",
    "1. Huffman encoder (Coppied from [here](http://www.techrepublic.com/article/huffman-coding-in-python/))\n",
    "2. Creation of prediction blocks and label for predictive coding\n",
    "3. Linear regression algorithm for seeing the baseline\n",
    "4. MLP algorithm (initial phase)\n",
    "\n",
    "**PS:** I made all of the tests with a single image (lena412.bmp). JPEG-ls algorithm achieves 5.21 bpp (bits per pixel) with huffman coding, JPEG-2000-ls achieves ~4.31 bpp. MLP is able to achieve ~4.5 bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1: Huffman encoder and Entropy Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binary tree data structure\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "class Node(object):\n",
    "\tleft = None\n",
    "\tright = None\n",
    "\titem = None\n",
    "\tweight = 0\n",
    "\n",
    "\tdef __init__(self, i, w):\n",
    "\t\tself.item = i\n",
    "\t\tself.weight = w\n",
    "\n",
    "\tdef setChildren(self, ln, rn):\n",
    "\t\tself.left = ln\n",
    "\t\tself.right = rn\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"%s - %s â€” %s _ %s\" % (self.item, self.weight, self.left, self.right)\n",
    "\n",
    "\tdef __cmp__(self, a):\n",
    "\t\treturn cmp(self.weight, a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Huffman Encoder\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "\n",
    "from itertools import groupby\n",
    "from heapq import *\n",
    "\n",
    "\n",
    "#Huffman encoder  \n",
    "def huffman(input):\n",
    "    itemqueue =  [Node(a,len(list(b))) for a,b in groupby(sorted(input))]\n",
    "    heapify(itemqueue)\n",
    "    while len(itemqueue) > 1:\n",
    "        l = heappop(itemqueue)\n",
    "        r = heappop(itemqueue)\n",
    "        n = Node(None, r.weight+l.weight)\n",
    "        n.setChildren(l,r)\n",
    "        heappush(itemqueue, n) \n",
    "        \n",
    "    codes = {}\n",
    "    def codeIt(s, node):\n",
    "        if node.item:\n",
    "            if not s:\n",
    "                codes[node.item] = \"0\"\n",
    "            else:\n",
    "                codes[node.item] = s\n",
    "        else:\n",
    "            codeIt(s+\"0\", node.left)\n",
    "            codeIt(s+\"1\", node.right)\n",
    "    codeIt(\"\",itemqueue[0])\n",
    "    return codes, \"\".join([codes[a] for a in input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitrate of the original image\n",
      "Bits per pixel is 7.46820831299 bpp\n"
     ]
    }
   ],
   "source": [
    "#Test Huffman encoder with an image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "img=mpimg.imread('lena512.bmp')\n",
    "#print(img.shape)\n",
    "#imgplot=plt.imshow(img,cmap='gray')\n",
    "\n",
    "img_input=img.reshape([-1]).astype(str)\n",
    "#print(img_input)\n",
    "huffman_img = huffman(img_input)\n",
    "#print(huffman_img[1])\n",
    "\n",
    "#print('Huffman code for ' + str(img) + ' is ' + str(huffman_img))\n",
    "#print('Original length is '+str(len(input) * 8)+', length of huffman coding is '+ str(len(huffman(input)[1])))\n",
    "print('Bitrate of the original image')\n",
    "print('Bits per pixel is ' + str(float(len(huffman_img[1])/float(len(img_input)))) + ' bpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def entropy(labels,degree):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    [counts, bins]  = np.histogram(labels,np.append(np.unique(labels),np.inf))\n",
    "    #print(bins)\n",
    "    probs = counts.astype(float) / float(n_labels)\n",
    "    #print(probs)\n",
    "    #n_classes = np.count_nonzero(probs)\n",
    "    \n",
    "    #if n_classes <= 1:\n",
    "        #return 0\n",
    "    \n",
    "\n",
    "    # Compute standard entropy.\n",
    "    if(degree==1):\n",
    "        ent = -np.sum(np.multiply(probs,np.log2(probs)))\n",
    "    else:\n",
    "        ent = np.log2(np.sum(np.power(probs,degree)))/(1-degree)\n",
    "    \n",
    "    '''ent = 0.\n",
    "    for i in probs:\n",
    "        if(degree==1):\n",
    "            ent -= i * np.log2(i)\n",
    "        else:\n",
    "            ent -= np.log2(np.sum(np.power()))'''\n",
    "\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.319146419\n",
      "-2.43295940728\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "x=[1.,1.,1.,1.,1.,1.,1.,1.,-7.,-7.,-7.,-7.,-7.,100.,100.,100.,-4.,-4.,-4.,5.]\n",
    "x=x+np.random.rand(20,1)\n",
    "x=np.asarray(x)\n",
    "#print(x)\n",
    "\n",
    "print(entropy(x,1))\n",
    "print(entropy(x,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Creation of prediction blocks and label for predictive coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lossless image copmpression using predictive coding. For reference see below\n",
    "#(https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "#Returns prediction blocks and the corresponding pixels in the image\n",
    "#Very naive implementation, neglects boundaries, can be improved further\n",
    "def pred_vectors(img,pred_size):\n",
    "    (n,m)=img.shape #image size\n",
    "    k,l=pred_size #Size of the predictive window\n",
    "    \n",
    "    fvec=np.zeros([(n-k-1)*(m-2*l),2*k*l+k+l])\n",
    "    #print(fvec.shape)\n",
    "    label = np.zeros([(n-k-1)*(m-2*l),1])\n",
    "    for (i,j) in product(range(k,n-1), range(l,m-l)):\n",
    "        #print(i,j)\n",
    "        idx = (i-k)*(m-2*l)+j-l\n",
    "        fvec_current =img[i-k:i,j-l:j+l+1].reshape([-1])\n",
    "        fvec_current = np.append(fvec_current,img[i,j-l:j].reshape([-1]))\n",
    "        fvec[idx,:]=fvec_current\n",
    "        label[idx]=img[i,j]\n",
    "        \n",
    "    return fvec, label\n",
    "\n",
    "\n",
    "\n",
    "fvec,label = pred_vectors(img,[3,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3: Linear regression algorithm for seeing the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with linear regression\n",
      "MSE is 33.4797180849\n",
      "Bits per pixel is 4.43469547481 bpp\n"
     ]
    }
   ],
   "source": [
    "#First trial: Simple regression network. No relation to deep learning just to gain some intuition\n",
    "\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "#Create the regression model using sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(fvec, label)\n",
    "\n",
    "#Predict and quantize the labels\n",
    "label_pred = np.round(regr.predict(fvec))\n",
    "\n",
    "#Calculate the error\n",
    "err=label_pred-label;\n",
    "\n",
    "print('Results with linear regression')\n",
    "#MSE\n",
    "print('MSE is '  + str(np.mean(err**2)))\n",
    "\n",
    "#Calculate Huffman coding of the error\n",
    "huffman_err = huffman(err.reshape([-1]).astype(str))\n",
    "print('Bits per pixel is ' + str(float(len(huffman_err[1])/float(len(err)))) + ' bpp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4: MLP algorithm (initial phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second trial: MLP\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes)-1):\n",
    "        layer_name=\"weights\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[k]])\n",
    "            b = tf.get_variable('b', shape=[hidden_sizes[k]])\n",
    "            x = activation_fn(tf.matmul(x, W) + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[-1]])\n",
    "        b = tf.get_variable('b', shape=[hidden_sizes[-1]])\n",
    "        return tf.matmul(x, W) + b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 9)\n",
      "array([[ 2.46076798,  4.04637003,  4.04785681]], dtype=float32)\n",
      "3.5183315\n"
     ]
    }
   ],
   "source": [
    "##Test for Renyi's Entropy\n",
    "\n",
    "from pprint import pprint\n",
    "#x=np.array([[1.,1.,1.,1.,1.,1.,1.,1.,2.,2.,2.,2.,2.,3.,3.,3.,4.,4.,5.,5.]])\n",
    "n=9\n",
    "x=np.array([[1,2,3,4,5,6,7,8,9],[1,3,28,10,23,12,43,1,9],[-4,0,2,1,54,1,3,8,-4]])\n",
    "x=x.astype(np.float32)\n",
    "\n",
    "c=np.power(n,-.2)\n",
    "print(x.shape)\n",
    "err_ = tf.placeholder(tf.float32, [None, n])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#x_std=tf.reduce_mean(x_)\n",
    "x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "h_=tf.reshape(tf.multiply(1.06,tf.multiply(x_std,c)),[-1,1])\n",
    "\n",
    "\n",
    "\n",
    "#prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "#Calculate Entropy\n",
    "prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "#Calculate Entropy\n",
    "sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "for k in range(1,100):\n",
    "    #print(k)\n",
    "    prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "    prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "    #Calculate Entropy\n",
    "    sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "loss=tf.reduce_mean(renyi_ent2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    f_val=sess.run(renyi_ent2, feed_dict={err_:x})\n",
    "    f_val2=sess.run(loss, feed_dict={err_:x})\n",
    "    pprint(f_val)\n",
    "    pprint(f_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252984, 52)\n"
     ]
    }
   ],
   "source": [
    "pprint(fvec_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t mse loss: 0.19890\t entropy1: 5.04256\t entropy2: 4.80390\t Huffman bitrate is 5.080\n",
      "iteration 2000\t mse loss: 0.05796\t entropy1: 5.88511\t entropy2: 5.70275\t Huffman bitrate is 5.950\n",
      "iteration 4000\t mse loss: 0.06422\t entropy1: 5.74021\t entropy2: 5.35698\t Huffman bitrate is 5.810\n",
      "iteration 6000\t mse loss: 0.02628\t entropy1: 5.03464\t entropy2: 4.62950\t Huffman bitrate is 5.070\n",
      "iteration 8000\t mse loss: 0.07235\t entropy1: 5.61085\t entropy2: 5.31043\t Huffman bitrate is 5.660\n",
      "iteration 10000\t mse loss: 0.04673\t entropy1: 5.81101\t entropy2: 5.64386\t Huffman bitrate is 5.870\n",
      "iteration 12000\t mse loss: 0.03350\t entropy1: 5.36837\t entropy2: 5.02093\t Huffman bitrate is 5.420\n",
      "iteration 14000\t mse loss: 0.05526\t entropy1: 6.02511\t entropy2: 5.77992\t Huffman bitrate is 6.090\n",
      "iteration 16000\t mse loss: 0.02354\t entropy1: 4.91027\t entropy2: 4.48681\t Huffman bitrate is 4.960\n",
      "iteration 18000\t mse loss: 0.02371\t entropy1: 5.12047\t entropy2: 4.82011\t Huffman bitrate is 5.170\n",
      "iteration 20000\t mse loss: 0.05978\t entropy1: 5.66611\t entropy2: 5.48036\t Huffman bitrate is 5.730\n",
      "iteration 22000\t mse loss: 0.03050\t entropy1: 5.09185\t entropy2: 4.71028\t Huffman bitrate is 5.140\n",
      "iteration 24000\t mse loss: 0.05676\t entropy1: 5.97101\t entropy2: 5.79586\t Huffman bitrate is 6.030\n",
      "iteration 26000\t mse loss: 0.02184\t entropy1: 5.41908\t entropy2: 5.14816\t Huffman bitrate is 5.460\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-23b8abd246f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m#print(err_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mtest_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-23b8abd246f9>\u001b[0m in \u001b[0;36mtest_classification\u001b[0;34m(model_function, learning_rate)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_value3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             ''' \n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# test trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##DOES NOT WORK!!!!!!!!!!!\n",
    "#TRIAL WITH RENYIS ENTROPY\n",
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of wron initialization\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "\n",
    "n=fvec_n.shape[1]\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #RENYI ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            #loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            \"\"\"\n",
    "            err_=tf.subtract(y_,y_logits)\n",
    "            x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "            x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "            h_=tf.reshape(tf.multiply(1.06,tf.multiply((x_std+.001),c)),[-1,1])\n",
    "\n",
    "\n",
    "\n",
    "            #prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "            #Calculate Entropy\n",
    "            prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "            prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "            #Calculate Entropy\n",
    "            sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "            for k in range(1,100):\n",
    "                #print(k)\n",
    "                prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "                prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "                #Calculate Entropy\n",
    "                sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "            renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "            loss=tf.reduce_mean(renyi_ent2)\n",
    "            \"\"\"\n",
    "            \n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        batch_size=100\n",
    "        ids=[i for i in range(batch_size)]\n",
    "        for iter_i in range(100001):\n",
    "            #print(iter_i)\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            ids=[(ids[0]+batch_size+i)%label.shape[0] for i in range(batch_size)]\n",
    "            tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "            '''\n",
    "            acc_value = sess.run(h_, feed_dict=tf_feed_dict)\n",
    "            acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "            acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "            y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "            err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys))\n",
    "            print(acc_value)\n",
    "            print(acc_value2)\n",
    "            print(acc_value3)\n",
    "            ''' \n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                #tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "                '''\n",
    "                acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "                acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys))\n",
    "                print(acc_value)\n",
    "                print(acc_value2)\n",
    "                print(acc_value3)\n",
    "                print(err_value.reshape([-1]))\n",
    "                '''\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "        #err_value =  np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "        #print(err_value)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t mse loss: 0.63563\t entropy1: 7.44656\t entropy2: 7.32115\t Huffman bitrate is 7.470\n",
      "iteration 2000\t mse loss: 0.03646\t entropy1: 5.11033\t entropy2: 4.50077\t Huffman bitrate is 5.160\n",
      "iteration 4000\t mse loss: 0.03272\t entropy1: 5.14986\t entropy2: 4.64941\t Huffman bitrate is 5.206\n",
      "iteration 6000\t mse loss: 0.02798\t entropy1: 4.86304\t entropy2: 4.31970\t Huffman bitrate is 4.937\n",
      "iteration 8000\t mse loss: 0.01996\t entropy1: 4.70085\t entropy2: 4.13966\t Huffman bitrate is 4.826\n",
      "iteration 10000\t mse loss: 0.02057\t entropy1: 4.63546\t entropy2: 4.08668\t Huffman bitrate is 4.750\n",
      "iteration 12000\t mse loss: 0.02396\t entropy1: 4.83564\t entropy2: 4.39802\t Huffman bitrate is 4.915\n",
      "iteration 14000\t mse loss: 0.01724\t entropy1: 4.54542\t entropy2: 4.01856\t Huffman bitrate is 4.687\n",
      "iteration 16000\t mse loss: 0.02156\t entropy1: 4.61002\t entropy2: 4.05941\t Huffman bitrate is 4.710\n",
      "iteration 18000\t mse loss: 0.01809\t entropy1: 4.57681\t entropy2: 4.03880\t Huffman bitrate is 4.713\n",
      "iteration 20000\t mse loss: 0.01684\t entropy1: 4.49395\t entropy2: 3.96815\t Huffman bitrate is 4.625\n",
      "iteration 22000\t mse loss: 0.02370\t entropy1: 4.53183\t entropy2: 4.00691\t Huffman bitrate is 4.607\n",
      "iteration 24000\t mse loss: 0.01677\t entropy1: 4.51010\t entropy2: 3.98177\t Huffman bitrate is 4.652\n",
      "iteration 26000\t mse loss: 0.02159\t entropy1: 4.64390\t entropy2: 4.18530\t Huffman bitrate is 4.740\n",
      "iteration 28000\t mse loss: 0.02744\t entropy1: 4.76793\t entropy2: 4.34797\t Huffman bitrate is 4.832\n",
      "iteration 30000\t mse loss: 0.03778\t entropy1: 4.87204\t entropy2: 4.48547\t Huffman bitrate is 4.918\n",
      "iteration 32000\t mse loss: 0.01787\t entropy1: 4.50898\t entropy2: 4.01484\t Huffman bitrate is 4.641\n",
      "iteration 34000\t mse loss: 0.01821\t entropy1: 4.56251\t entropy2: 4.08200\t Huffman bitrate is 4.685\n",
      "iteration 36000\t mse loss: 0.02106\t entropy1: 4.46509\t entropy2: 3.93890\t Huffman bitrate is 4.554\n",
      "iteration 38000\t mse loss: 0.01821\t entropy1: 4.59729\t entropy2: 4.17082\t Huffman bitrate is 4.715\n",
      "iteration 40000\t mse loss: 0.03012\t entropy1: 4.71881\t entropy2: 4.29974\t Huffman bitrate is 4.777\n",
      "iteration 42000\t mse loss: 0.02650\t entropy1: 4.59768\t entropy2: 4.14368\t Huffman bitrate is 4.673\n",
      "iteration 44000\t mse loss: 0.01598\t entropy1: 4.44624\t entropy2: 3.93846\t Huffman bitrate is 4.590\n",
      "iteration 46000\t mse loss: 0.01627\t entropy1: 4.46076\t entropy2: 3.94937\t Huffman bitrate is 4.596\n",
      "iteration 48000\t mse loss: 0.03460\t entropy1: 4.68421\t entropy2: 4.27529\t Huffman bitrate is 4.736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6523f7b5e517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m#print(err_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mtest_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-6523f7b5e517>\u001b[0m in \u001b[0;36mtest_classification\u001b[0;34m(model_function, learning_rate)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# test trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##WORKING CODE WITH MSE\n",
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of wron initialization\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            #loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #SHANNON ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(100001):\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            ids=[(ids[0]+100+i)%label.shape[0] for i in range(100)]\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "                #print(err_value.reshape([-1]))\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "        err_value =  np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "        #print(err_value)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
