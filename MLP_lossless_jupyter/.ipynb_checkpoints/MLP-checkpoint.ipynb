{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Image Compression Project MLP part\n",
    "\n",
    "This code applies predictive coding algoritm for lossless image compression with a basic MLP structure. Details of predictive coding algorithm can be found [here](https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "The code has four parts\n",
    "\n",
    "1. Huffman encoder (Coppied from [here](http://www.techrepublic.com/article/huffman-coding-in-python/))\n",
    "2. Creation of prediction blocks and label for predictive coding\n",
    "3. Linear regression algorithm for seeing the baseline\n",
    "4. MLP algorithm (initial phase)\n",
    "\n",
    "**PS:** I made all of the tests with a single image (lena412.bmp). JPEG-ls algorithm achieves 5.21 bpp (bits per pixel) with huffman coding, JPEG-2000-ls achieves ~4.31 bpp. MLP is able to achieve ~4.5 bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1: Huffman encoder and Entropy Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binary tree data structure\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "class Node(object):\n",
    "\tleft = None\n",
    "\tright = None\n",
    "\titem = None\n",
    "\tweight = 0\n",
    "\n",
    "\tdef __init__(self, i, w):\n",
    "\t\tself.item = i\n",
    "\t\tself.weight = w\n",
    "\n",
    "\tdef setChildren(self, ln, rn):\n",
    "\t\tself.left = ln\n",
    "\t\tself.right = rn\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"%s - %s â€” %s _ %s\" % (self.item, self.weight, self.left, self.right)\n",
    "\n",
    "\tdef __cmp__(self, a):\n",
    "\t\treturn cmp(self.weight, a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Huffman Encoder\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "\n",
    "from itertools import groupby\n",
    "from heapq import *\n",
    "\n",
    "\n",
    "#Huffman encoder  \n",
    "def huffman(input):\n",
    "    itemqueue =  [Node(a,len(list(b))) for a,b in groupby(sorted(input))]\n",
    "    heapify(itemqueue)\n",
    "    while len(itemqueue) > 1:\n",
    "        l = heappop(itemqueue)\n",
    "        r = heappop(itemqueue)\n",
    "        n = Node(None, r.weight+l.weight)\n",
    "        n.setChildren(l,r)\n",
    "        heappush(itemqueue, n) \n",
    "        \n",
    "    codes = {}\n",
    "    def codeIt(s, node):\n",
    "        if node.item:\n",
    "            if not s:\n",
    "                codes[node.item] = \"0\"\n",
    "            else:\n",
    "                codes[node.item] = s\n",
    "        else:\n",
    "            codeIt(s+\"0\", node.left)\n",
    "            codeIt(s+\"1\", node.right)\n",
    "    codeIt(\"\",itemqueue[0])\n",
    "    return codes, \"\".join([codes[a] for a in input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitrate of the original image\n",
      "Bits per pixel is 7.46820831299 bpp\n"
     ]
    }
   ],
   "source": [
    "#Test Huffman encoder with an image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "img=mpimg.imread('lena512.bmp')\n",
    "#print(img.shape)\n",
    "#imgplot=plt.imshow(img,cmap='gray')\n",
    "\n",
    "img_input=img.reshape([-1]).astype(str)\n",
    "#print(img_input)\n",
    "huffman_img = huffman(img_input)\n",
    "#print(huffman_img[1])\n",
    "\n",
    "#print('Huffman code for ' + str(img) + ' is ' + str(huffman_img))\n",
    "#print('Original length is '+str(len(input) * 8)+', length of huffman coding is '+ str(len(huffman(input)[1])))\n",
    "print('Bitrate of the original image')\n",
    "print('Bits per pixel is ' + str(float(len(huffman_img[1])/float(len(img_input)))) + ' bpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def entropy(labels,degree):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    [counts, bins]  = np.histogram(labels,np.append(np.unique(labels),np.inf))\n",
    "    #print(bins)\n",
    "    probs = counts.astype(float) / float(n_labels)\n",
    "    #print(probs)\n",
    "    #n_classes = np.count_nonzero(probs)\n",
    "    \n",
    "    #if n_classes <= 1:\n",
    "        #return 0\n",
    "    \n",
    "\n",
    "    # Compute standard entropy.\n",
    "    if(degree==1):\n",
    "        ent = -np.sum(np.multiply(probs,np.log2(probs)))\n",
    "    else:\n",
    "        ent = np.log2(np.sum(np.power(probs,degree)))/(1-degree)\n",
    "    \n",
    "    '''ent = 0.\n",
    "    for i in probs:\n",
    "        if(degree==1):\n",
    "            ent -= i * np.log2(i)\n",
    "        else:\n",
    "            ent -= np.log2(np.sum(np.power()))'''\n",
    "\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.06595732095\n",
      "1.88896868761\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "x=[1.,1.,1.,1.,1.,1.,1.,1.,-7.,-7.,-7.,-7.,-7.,100.,100.,100.,-4.,-4.,-4.,5.]\n",
    "x=np.asarray(x)\n",
    "#print(x)\n",
    "print(entropy(x,1))\n",
    "print(entropy(x,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Creation of prediction blocks and label for predictive coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lossless image copmpression using predictive coding. For reference see below\n",
    "#(https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "#Returns prediction blocks and the corresponding pixels in the image\n",
    "#Very naive implementation, neglects boundaries, can be improved further\n",
    "def pred_vectors(img,pred_size):\n",
    "    (n,m)=img.shape #image size\n",
    "    k,l=pred_size #Size of the predictive window\n",
    "    \n",
    "    fvec=np.zeros([(n-k-1)*(m-2*l),2*k*l+k+l])\n",
    "    #print(fvec.shape)\n",
    "    label = np.zeros([(n-k-1)*(m-2*l),1])\n",
    "    for (i,j) in product(range(k,n-1), range(l,m-l)):\n",
    "        #print(i,j)\n",
    "        idx = (i-k)*(m-2*l)+j-l\n",
    "        fvec_current =img[i-k:i,j-l:j+l+1].reshape([-1])\n",
    "        fvec_current = np.append(fvec_current,img[i,j-l:j].reshape([-1]))\n",
    "        fvec[idx,:]=fvec_current\n",
    "        label[idx]=img[i,j]\n",
    "        \n",
    "    return fvec, label\n",
    "\n",
    "\n",
    "\n",
    "fvec,label = pred_vectors(img,[3,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3: Linear regression algorithm for seeing the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with linear regression\n",
      "MSE is 33.4797180849\n",
      "Bits per pixel is 4.43469547481 bpp\n"
     ]
    }
   ],
   "source": [
    "#First trial: Simple regression network. No relation to deep learning just to gain some intuition\n",
    "\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "#Create the regression model using sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(fvec, label)\n",
    "\n",
    "#Predict and quantize the labels\n",
    "label_pred = np.round(regr.predict(fvec))\n",
    "\n",
    "#Calculate the error\n",
    "err=label_pred-label;\n",
    "\n",
    "print('Results with linear regression')\n",
    "#MSE\n",
    "print('MSE is '  + str(np.mean(err**2)))\n",
    "\n",
    "#Calculate Huffman coding of the error\n",
    "huffman_err = huffman(err.reshape([-1]).astype(str))\n",
    "print('Bits per pixel is ' + str(float(len(huffman_err[1])/float(len(err)))) + ' bpp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4: MLP algorithm (initial phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second trial: MLP\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes)-1):\n",
    "        layer_name=\"weights\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[k]])\n",
    "            #b = tf.get_variable('b', shape=[hidden_sizes[k]])\n",
    "            x = activation_fn(tf.matmul(x, W))# + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[-1]])\n",
    "        #b = tf.get_variable('b', shape=[hidden_sizes[-1]])\n",
    "        return tf.matmul(x, W)# + b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t mse loss: 0.50735\t entropy1: 7.45237\t entropy2: 7.32331\t Huffman bitrate is 7.476\n",
      "iteration 2000\t mse loss: 0.04258\t entropy1: 5.13438\t entropy2: 4.55420\t Huffman bitrate is 5.177\n",
      "iteration 4000\t mse loss: 0.03718\t entropy1: 4.99522\t entropy2: 4.42917\t Huffman bitrate is 5.042\n",
      "iteration 6000\t mse loss: 0.03237\t entropy1: 4.92294\t entropy2: 4.35963\t Huffman bitrate is 4.984\n",
      "iteration 8000\t mse loss: 0.02085\t entropy1: 4.67332\t entropy2: 4.04101\t Huffman bitrate is 4.788\n",
      "iteration 10000\t mse loss: 0.02166\t entropy1: 4.61387\t entropy2: 4.01683\t Huffman bitrate is 4.712\n",
      "iteration 12000\t mse loss: 0.01916\t entropy1: 4.58384\t entropy2: 3.99492\t Huffman bitrate is 4.700\n",
      "iteration 14000\t mse loss: 0.02523\t entropy1: 4.64903\t entropy2: 4.13555\t Huffman bitrate is 4.731\n",
      "iteration 16000\t mse loss: 0.02365\t entropy1: 4.62735\t entropy2: 4.09152\t Huffman bitrate is 4.714\n",
      "iteration 18000\t mse loss: 0.01997\t entropy1: 4.56938\t entropy2: 4.01134\t Huffman bitrate is 4.675\n",
      "iteration 20000\t mse loss: 0.01817\t entropy1: 4.49861\t entropy2: 3.95440\t Huffman bitrate is 4.613\n",
      "iteration 22000\t mse loss: 0.02556\t entropy1: 4.57632\t entropy2: 4.05205\t Huffman bitrate is 4.652\n",
      "iteration 24000\t mse loss: 0.02538\t entropy1: 4.66225\t entropy2: 4.16357\t Huffman bitrate is 4.744\n",
      "iteration 26000\t mse loss: 0.01788\t entropy1: 4.50575\t entropy2: 3.96272\t Huffman bitrate is 4.627\n",
      "iteration 28000\t mse loss: 0.02681\t entropy1: 4.65587\t entropy2: 4.16801\t Huffman bitrate is 4.736\n",
      "iteration 30000\t mse loss: 0.01632\t entropy1: 4.46525\t entropy2: 3.92827\t Huffman bitrate is 4.609\n",
      "iteration 32000\t mse loss: 0.02538\t entropy1: 4.62328\t entropy2: 4.15495\t Huffman bitrate is 4.703\n",
      "iteration 34000\t mse loss: 0.01620\t entropy1: 4.45159\t entropy2: 3.91604\t Huffman bitrate is 4.594\n",
      "iteration 36000\t mse loss: 0.02857\t entropy1: 4.58961\t entropy2: 4.10014\t Huffman bitrate is 4.657\n",
      "iteration 38000\t mse loss: 0.02454\t entropy1: 4.51981\t entropy2: 4.03052\t Huffman bitrate is 4.599\n",
      "iteration 40000\t mse loss: 0.02051\t entropy1: 4.56584\t entropy2: 4.04836\t Huffman bitrate is 4.667\n",
      "iteration 42000\t mse loss: 0.02406\t entropy1: 4.54511\t entropy2: 4.06558\t Huffman bitrate is 4.629\n",
      "iteration 44000\t mse loss: 0.01615\t entropy1: 4.44665\t entropy2: 3.92317\t Huffman bitrate is 4.589\n",
      "iteration 46000\t mse loss: 0.02667\t entropy1: 4.55503\t entropy2: 4.06959\t Huffman bitrate is 4.628\n",
      "iteration 48000\t mse loss: 0.03980\t entropy1: 4.72856\t entropy2: 4.34621\t Huffman bitrate is 4.770\n",
      "iteration 50000\t mse loss: 0.01803\t entropy1: 4.44064\t entropy2: 3.93328\t Huffman bitrate is 4.554\n",
      "iteration 52000\t mse loss: 0.01642\t entropy1: 4.40678\t entropy2: 3.89734\t Huffman bitrate is 4.533\n",
      "iteration 54000\t mse loss: 0.01560\t entropy1: 4.40426\t entropy2: 3.89397\t Huffman bitrate is 4.547\n",
      "iteration 56000\t mse loss: 0.01963\t entropy1: 4.48060\t entropy2: 3.97369\t Huffman bitrate is 4.581\n",
      "iteration 58000\t mse loss: 0.02794\t entropy1: 4.52956\t entropy2: 4.08845\t Huffman bitrate is 4.602\n",
      "iteration 60000\t mse loss: 0.01707\t entropy1: 4.40786\t entropy2: 3.89764\t Huffman bitrate is 4.527\n",
      "iteration 62000\t mse loss: 0.02566\t entropy1: 4.49938\t entropy2: 4.05665\t Huffman bitrate is 4.577\n",
      "iteration 64000\t mse loss: 0.02350\t entropy1: 4.46474\t entropy2: 3.98960\t Huffman bitrate is 4.545\n",
      "iteration 66000\t mse loss: 0.01817\t entropy1: 4.44109\t entropy2: 3.93263\t Huffman bitrate is 4.556\n",
      "iteration 68000\t mse loss: 0.03215\t entropy1: 4.61007\t entropy2: 4.18120\t Huffman bitrate is 4.671\n",
      "iteration 70000\t mse loss: 0.01530\t entropy1: 4.37866\t entropy2: 3.87567\t Huffman bitrate is 4.524\n",
      "iteration 72000\t mse loss: 0.02543\t entropy1: 4.47153\t entropy2: 3.99856\t Huffman bitrate is 4.548\n",
      "iteration 74000\t mse loss: 0.01615\t entropy1: 4.39019\t entropy2: 3.89615\t Huffman bitrate is 4.517\n",
      "iteration 76000\t mse loss: 0.02054\t entropy1: 4.44592\t entropy2: 3.95362\t Huffman bitrate is 4.534\n",
      "iteration 78000\t mse loss: 0.02453\t entropy1: 4.48228\t entropy2: 4.03240\t Huffman bitrate is 4.562\n",
      "iteration 80000\t mse loss: 0.01538\t entropy1: 4.38725\t entropy2: 3.88501\t Huffman bitrate is 4.533\n",
      "iteration 82000\t mse loss: 0.02601\t entropy1: 4.54274\t entropy2: 4.09184\t Huffman bitrate is 4.623\n",
      "iteration 84000\t mse loss: 0.03512\t entropy1: 4.60774\t entropy2: 4.19838\t Huffman bitrate is 4.663\n",
      "iteration 86000\t mse loss: 0.01684\t entropy1: 4.38082\t entropy2: 3.89333\t Huffman bitrate is 4.499\n",
      "iteration 88000\t mse loss: 0.01535\t entropy1: 4.38743\t entropy2: 3.89862\t Huffman bitrate is 4.531\n",
      "iteration 90000\t mse loss: 0.01589\t entropy1: 4.39333\t entropy2: 3.89630\t Huffman bitrate is 4.530\n",
      "iteration 92000\t mse loss: 0.01737\t entropy1: 4.51453\t entropy2: 4.03703\t Huffman bitrate is 4.641\n",
      "iteration 94000\t mse loss: 0.02410\t entropy1: 4.49350\t entropy2: 4.04268\t Huffman bitrate is 4.575\n",
      "iteration 96000\t mse loss: 0.01874\t entropy1: 4.40329\t entropy2: 3.93996\t Huffman bitrate is 4.506\n",
      "iteration 98000\t mse loss: 0.02139\t entropy1: 4.46552\t entropy2: 3.99000\t Huffman bitrate is 4.555\n",
      "iteration 100000\t mse loss: 0.01706\t entropy1: 4.37963\t entropy2: 3.89747\t Huffman bitrate is 4.496\n"
     ]
    }
   ],
   "source": [
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of wron initialization\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            #loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #SHANNON ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(100001):\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            ids=[(ids[0]+100+i)%label.shape[0] for i in range(100)]\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "                #print(err_value.reshape([-1]))\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "        err_value =  np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "        #print(err_value)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
