{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Image Compression Project MLP part\n",
    "\n",
    "This code applies predictive coding algoritm for lossless image compression with a basic MLP structure. Details of predictive coding algorithm can be found [here](https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "The code has four parts\n",
    "\n",
    "1. Huffman encoder (Coppied from [here](http://www.techrepublic.com/article/huffman-coding-in-python/))\n",
    "2. Creation of prediction blocks and label for predictive coding\n",
    "3. Linear regression algorithm for seeing the baseline\n",
    "4. MLP algorithm (initial phase)\n",
    "\n",
    "**PS:** I made all of the tests with a single image (lena412.bmp). JPEG-ls algorithm achieves 5.21 bpp (bits per pixel) with huffman coding, JPEG-2000-ls achieves ~4.31 bpp. MLP is able to achieve ~4.5 bpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1: Huffman encoder and Entropy Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Binary tree data structure\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "class Node(object):\n",
    "\tleft = None\n",
    "\tright = None\n",
    "\titem = None\n",
    "\tweight = 0\n",
    "\n",
    "\tdef __init__(self, i, w):\n",
    "\t\tself.item = i\n",
    "\t\tself.weight = w\n",
    "\n",
    "\tdef setChildren(self, ln, rn):\n",
    "\t\tself.left = ln\n",
    "\t\tself.right = rn\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\treturn \"%s - %s â€” %s _ %s\" % (self.item, self.weight, self.left, self.right)\n",
    "\n",
    "\tdef __cmp__(self, a):\n",
    "\t\treturn cmp(self.weight, a.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Huffman Encoder\n",
    "#http://www.techrepublic.com/article/huffman-coding-in-python/\n",
    "\n",
    "from itertools import groupby\n",
    "from heapq import *\n",
    "\n",
    "\n",
    "#Huffman encoder  \n",
    "def huffman(input):\n",
    "    itemqueue =  [Node(a,len(list(b))) for a,b in groupby(sorted(input))]\n",
    "    heapify(itemqueue)\n",
    "    while len(itemqueue) > 1:\n",
    "        l = heappop(itemqueue)\n",
    "        r = heappop(itemqueue)\n",
    "        n = Node(None, r.weight+l.weight)\n",
    "        n.setChildren(l,r)\n",
    "        heappush(itemqueue, n) \n",
    "        \n",
    "    codes = {}\n",
    "    def codeIt(s, node):\n",
    "        if node.item:\n",
    "            if not s:\n",
    "                codes[node.item] = \"0\"\n",
    "            else:\n",
    "                codes[node.item] = s\n",
    "        else:\n",
    "            codeIt(s+\"0\", node.left)\n",
    "            codeIt(s+\"1\", node.right)\n",
    "    codeIt(\"\",itemqueue[0])\n",
    "    return codes, \"\".join([codes[a] for a in input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitrate of the original image\n",
      "Bits per pixel is 7.46820831299 bpp\n"
     ]
    }
   ],
   "source": [
    "#Test Huffman encoder with an image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "img=mpimg.imread('lena512.bmp')\n",
    "#print(img.shape)\n",
    "#imgplot=plt.imshow(img,cmap='gray')\n",
    "\n",
    "img_input=img.reshape([-1]).astype(str)\n",
    "#print(img_input)\n",
    "huffman_img = huffman(img_input)\n",
    "#print(huffman_img[1])\n",
    "\n",
    "#print('Huffman code for ' + str(img) + ' is ' + str(huffman_img))\n",
    "#print('Original length is '+str(len(input) * 8)+', length of huffman coding is '+ str(len(huffman(input)[1])))\n",
    "print('Bitrate of the original image')\n",
    "print('Bits per pixel is ' + str(float(len(huffman_img[1])/float(len(img_input)))) + ' bpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def entropy(labels,degree):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    [counts, bins]  = np.histogram(labels,np.append(np.unique(labels),np.inf))\n",
    "    #print(bins)\n",
    "    probs = counts.astype(float) / float(n_labels)\n",
    "    #print(probs)\n",
    "    #n_classes = np.count_nonzero(probs)\n",
    "    \n",
    "    #if n_classes <= 1:\n",
    "        #return 0\n",
    "    \n",
    "\n",
    "    # Compute standard entropy.\n",
    "    if(degree==1):\n",
    "        ent = -np.sum(np.multiply(probs,np.log2(probs)))\n",
    "    else:\n",
    "        ent = np.log2(np.sum(np.power(probs,degree)))/(1-degree)\n",
    "    \n",
    "    '''ent = 0.\n",
    "    for i in probs:\n",
    "        if(degree==1):\n",
    "            ent -= i * np.log2(i)\n",
    "        else:\n",
    "            ent -= np.log2(np.sum(np.power()))'''\n",
    "\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.319146419\n",
      "-2.43295940728\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "x=[1.,1.,1.,1.,1.,1.,1.,1.,-7.,-7.,-7.,-7.,-7.,100.,100.,100.,-4.,-4.,-4.,5.]\n",
    "x=x+np.random.rand(20,1)\n",
    "x=np.asarray(x)\n",
    "#print(x)\n",
    "\n",
    "print(entropy(x,1))\n",
    "print(entropy(x,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: Creation of prediction blocks and label for predictive coding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lossless image copmpression using predictive coding. For reference see below\n",
    "#(https://web.stanford.edu/class/ee398a/handouts/lectures/06-Prediction.pdf)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "#Returns prediction blocks and the corresponding pixels in the image\n",
    "#Very naive implementation, neglects boundaries, can be improved further\n",
    "def pred_vectors(img,pred_size):\n",
    "    (n,m)=img.shape #image size\n",
    "    k,l=pred_size #Size of the predictive window\n",
    "    \n",
    "    fvec=np.zeros([(n-k-1)*(m-2*l),2*k*l+k+l])\n",
    "    #print(fvec.shape)\n",
    "    label = np.zeros([(n-k-1)*(m-2*l),1])\n",
    "    for (i,j) in product(range(k,n-1), range(l,m-l)):\n",
    "        #print(i,j)\n",
    "        idx = (i-k)*(m-2*l)+j-l\n",
    "        fvec_current =img[i-k:i,j-l:j+l+1].reshape([-1])\n",
    "        fvec_current = np.append(fvec_current,img[i,j-l:j].reshape([-1]))\n",
    "        fvec[idx,:]=fvec_current\n",
    "        label[idx]=img[i,j]\n",
    "        \n",
    "    return fvec, label\n",
    "\n",
    "\n",
    "\n",
    "fvec,label = pred_vectors(img,[5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3: Linear regression algorithm for seeing the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with linear regression\n",
      "MSE is 33.6809051251\n",
      "Bits per pixel is 4.41969354764 bpp\n"
     ]
    }
   ],
   "source": [
    "#First trial: Simple regression network. No relation to deep learning just to gain some intuition\n",
    "\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "\n",
    "#Create the regression model using sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(fvec, label)\n",
    "\n",
    "#Predict and quantize the labels\n",
    "label_pred = np.round(regr.predict(fvec))\n",
    "\n",
    "#Calculate the error\n",
    "err=label_pred-label;\n",
    "\n",
    "print('Results with linear regression')\n",
    "#MSE\n",
    "print('MSE is '  + str(np.mean(err**2)))\n",
    "\n",
    "#Calculate Huffman coding of the error\n",
    "huffman_err = huffman(err.reshape([-1]).astype(str))\n",
    "print('Bits per pixel is ' + str(float(len(huffman_err[1])/float(len(err)))) + ' bpp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4: MLP algorithm (initial phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second trial: MLP\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu,dropout_rate=1.0,std_dev=1.0):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=std_dev)}\n",
    "    for k in range(len(hidden_sizes)-1):\n",
    "        layer_name=\"weights\"+str(k)\n",
    "        #FC layers\n",
    "        with tf.variable_scope(layer_name, **scope_args):\n",
    "            W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[k]])\n",
    "            #b = tf.get_variable('b', shape=[hidden_sizes[k]])\n",
    "            x = activation_fn(tf.matmul(x, W))# + b)\n",
    "            #Dropout before the last layer\n",
    "            x = tf.nn.dropout(x, keep_prob=dropout_rate)\n",
    "    #Softmax layer\n",
    "    with tf.variable_scope('outlayer', **scope_args):\n",
    "        W = tf.get_variable('W', shape=[x.shape[-1], hidden_sizes[-1]])\n",
    "        #b = tf.get_variable('b', shape=[hidden_sizes[-1]])\n",
    "        return tf.matmul(x, W)# + b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9)\n",
      "array([[ 2.45967555]], dtype=float32)\n",
      "2.4596756\n"
     ]
    }
   ],
   "source": [
    "##Test for Renyi's Entropy\n",
    "\n",
    "from pprint import pprint\n",
    "#x=np.array([[1.,1.,1.,1.,1.,1.,1.,1.,2.,2.,2.,2.,2.,3.,3.,3.,4.,4.,5.,5.]])\n",
    "n=9\n",
    "x=np.array([[1.2,2.7,3.1,4.65,5.32,6.123,7.21312,8.8,9.12]])#,\n",
    "#            [1,3,28,10,23,12,43,1,9],[-4,0,2,1,54,1,3,8,-4]])\n",
    "x=x.astype(np.float32)\n",
    "\n",
    "c=np.power(n,-.2)\n",
    "print(x.shape)\n",
    "err_ = tf.placeholder(tf.float32, [None, n])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#x_std=tf.reduce_mean(x_)\n",
    "x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "h_=tf.reshape(tf.multiply(1.06,tf.multiply(x_std,c)),[-1,1])\n",
    "\n",
    "\n",
    "\n",
    "#prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "#Calculate Entropy\n",
    "prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "#Calculate Entropy\n",
    "sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "for k in range(1,100):\n",
    "    #print(k)\n",
    "    prob_mat = tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+50),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "    prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "    #Calculate Entropy\n",
    "    sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "loss=tf.reduce_mean(renyi_ent2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    f_val=sess.run(renyi_ent2, feed_dict={err_:x})\n",
    "    f_val2=sess.run(loss, feed_dict={err_:x})\n",
    "    pprint(f_val)\n",
    "    pprint(f_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(254012, 60)\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "pprint(fvec_n.shape)\n",
    "batch_size=128\n",
    "batch_cross = .5\n",
    "batch_inc = int(batch_size*batch_cross)\n",
    "print(batch_inc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4-a: MLP with MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\t mse loss: 0.50683\t entropy1: 7.44758\t entropy2: 7.32382\t Huffman bitrate is 7.471\n",
      "iteration 100\t mse loss: 0.04009\t entropy1: 5.56038\t entropy2: 4.78043\t Huffman bitrate is 5.629\n",
      "iteration 200\t mse loss: 0.05668\t entropy1: 5.76275\t entropy2: 5.11141\t Huffman bitrate is 5.812\n",
      "iteration 300\t mse loss: 0.03817\t entropy1: 5.57014\t entropy2: 4.79643\t Huffman bitrate is 5.652\n",
      "iteration 400\t mse loss: 0.05775\t entropy1: 5.75165\t entropy2: 5.11708\t Huffman bitrate is 5.800\n",
      "iteration 500\t mse loss: 0.03785\t entropy1: 5.49867\t entropy2: 4.72286\t Huffman bitrate is 5.573\n",
      "iteration 600\t mse loss: 0.07210\t entropy1: 5.85609\t entropy2: 5.31388\t Huffman bitrate is 5.902\n",
      "iteration 700\t mse loss: 0.03962\t entropy1: 5.51076\t entropy2: 4.78333\t Huffman bitrate is 5.568\n",
      "iteration 800\t mse loss: 0.04534\t entropy1: 5.55753\t entropy2: 4.87710\t Huffman bitrate is 5.605\n",
      "iteration 900\t mse loss: 0.04473\t entropy1: 5.43035\t entropy2: 4.72875\t Huffman bitrate is 5.473\n",
      "iteration 1000\t mse loss: 0.03427\t entropy1: 5.35558\t entropy2: 4.61184\t Huffman bitrate is 5.437\n",
      "iteration 1100\t mse loss: 0.03153\t entropy1: 5.31425\t entropy2: 4.56679\t Huffman bitrate is 5.419\n",
      "iteration 1200\t mse loss: 0.04785\t entropy1: 5.49698\t entropy2: 4.87440\t Huffman bitrate is 5.542\n",
      "iteration 1300\t mse loss: 0.03399\t entropy1: 5.23627\t entropy2: 4.51534\t Huffman bitrate is 5.306\n",
      "iteration 1400\t mse loss: 0.05348\t entropy1: 5.47271\t entropy2: 4.91788\t Huffman bitrate is 5.517\n",
      "iteration 1500\t mse loss: 0.07173\t entropy1: 5.52635\t entropy2: 5.03489\t Huffman bitrate is 5.569\n",
      "iteration 1600\t mse loss: 0.07297\t entropy1: 5.52846\t entropy2: 5.05676\t Huffman bitrate is 5.572\n",
      "iteration 1700\t mse loss: 0.03435\t entropy1: 5.15738\t entropy2: 4.50561\t Huffman bitrate is 5.217\n",
      "iteration 1800\t mse loss: 0.03319\t entropy1: 5.12012\t entropy2: 4.46650\t Huffman bitrate is 5.183\n",
      "iteration 1900\t mse loss: 0.02556\t entropy1: 5.00260\t entropy2: 4.29418\t Huffman bitrate is 5.126\n",
      "iteration 2000\t mse loss: 0.05022\t entropy1: 5.21568\t entropy2: 4.66675\t Huffman bitrate is 5.253\n",
      "iteration 2100\t mse loss: 0.02379\t entropy1: 4.90519\t entropy2: 4.20369\t Huffman bitrate is 5.033\n",
      "iteration 2200\t mse loss: 0.02314\t entropy1: 4.88615\t entropy2: 4.18119\t Huffman bitrate is 5.021\n",
      "iteration 2300\t mse loss: 0.02732\t entropy1: 4.91561\t entropy2: 4.23209\t Huffman bitrate is 5.005\n",
      "iteration 2400\t mse loss: 0.02440\t entropy1: 4.88613\t entropy2: 4.19147\t Huffman bitrate is 4.999\n",
      "iteration 2500\t mse loss: 0.02323\t entropy1: 4.86729\t entropy2: 4.17101\t Huffman bitrate is 4.993\n",
      "iteration 2600\t mse loss: 0.04502\t entropy1: 5.15831\t entropy2: 4.61003\t Huffman bitrate is 5.197\n",
      "iteration 2700\t mse loss: 0.04648\t entropy1: 5.19698\t entropy2: 4.65612\t Huffman bitrate is 5.235\n",
      "iteration 2800\t mse loss: 0.02343\t entropy1: 4.90746\t entropy2: 4.20656\t Huffman bitrate is 5.041\n",
      "iteration 2900\t mse loss: 0.04715\t entropy1: 5.21258\t entropy2: 4.67701\t Huffman bitrate is 5.252\n",
      "iteration 3000\t mse loss: 0.03356\t entropy1: 5.02393\t entropy2: 4.40042\t Huffman bitrate is 5.084\n",
      "iteration 3100\t mse loss: 0.04508\t entropy1: 5.16476\t entropy2: 4.62310\t Huffman bitrate is 5.204\n",
      "iteration 3200\t mse loss: 0.02308\t entropy1: 4.87500\t entropy2: 4.18733\t Huffman bitrate is 5.004\n",
      "iteration 3300\t mse loss: 0.02402\t entropy1: 4.87136\t entropy2: 4.18302\t Huffman bitrate is 4.987\n",
      "iteration 3400\t mse loss: 0.03652\t entropy1: 4.98697\t entropy2: 4.37786\t Huffman bitrate is 5.043\n",
      "iteration 3500\t mse loss: 0.02300\t entropy1: 4.86725\t entropy2: 4.19328\t Huffman bitrate is 4.996\n",
      "iteration 3600\t mse loss: 0.02969\t entropy1: 4.88709\t entropy2: 4.24217\t Huffman bitrate is 4.965\n",
      "iteration 3700\t mse loss: 0.02643\t entropy1: 4.90098\t entropy2: 4.24917\t Huffman bitrate is 4.995\n",
      "iteration 3800\t mse loss: 0.04100\t entropy1: 5.06992\t entropy2: 4.52703\t Huffman bitrate is 5.113\n",
      "iteration 3900\t mse loss: 0.02345\t entropy1: 4.80617\t entropy2: 4.15223\t Huffman bitrate is 4.914\n",
      "iteration 4000\t mse loss: 0.08469\t entropy1: 5.56955\t entropy2: 5.22200\t Huffman bitrate is 5.612\n",
      "iteration 4100\t mse loss: 0.03189\t entropy1: 4.91620\t entropy2: 4.32440\t Huffman bitrate is 4.983\n",
      "iteration 4200\t mse loss: 0.02928\t entropy1: 4.83822\t entropy2: 4.21949\t Huffman bitrate is 4.913\n",
      "iteration 4300\t mse loss: 0.03091\t entropy1: 4.85381\t entropy2: 4.24121\t Huffman bitrate is 4.924\n",
      "iteration 4400\t mse loss: 0.02169\t entropy1: 4.76360\t entropy2: 4.10783\t Huffman bitrate is 4.888\n",
      "iteration 4500\t mse loss: 0.05763\t entropy1: 5.20428\t entropy2: 4.75352\t Huffman bitrate is 5.239\n",
      "iteration 4600\t mse loss: 0.02165\t entropy1: 4.74009\t entropy2: 4.09111\t Huffman bitrate is 4.858\n",
      "iteration 4700\t mse loss: 0.02069\t entropy1: 4.73146\t entropy2: 4.07501\t Huffman bitrate is 4.865\n",
      "iteration 4800\t mse loss: 0.02228\t entropy1: 4.73415\t entropy2: 4.08115\t Huffman bitrate is 4.845\n",
      "iteration 4900\t mse loss: 0.05041\t entropy1: 5.12661\t entropy2: 4.66312\t Huffman bitrate is 5.164\n",
      "iteration 5000\t mse loss: 0.02862\t entropy1: 4.82270\t entropy2: 4.22580\t Huffman bitrate is 4.900\n",
      "iteration 5100\t mse loss: 0.03138\t entropy1: 4.82965\t entropy2: 4.23265\t Huffman bitrate is 4.898\n",
      "iteration 5200\t mse loss: 0.03233\t entropy1: 4.90379\t entropy2: 4.32971\t Huffman bitrate is 4.969\n",
      "iteration 5300\t mse loss: 0.02080\t entropy1: 4.76067\t entropy2: 4.10042\t Huffman bitrate is 4.902\n",
      "iteration 5400\t mse loss: 0.04637\t entropy1: 5.12143\t entropy2: 4.63481\t Huffman bitrate is 5.160\n",
      "iteration 5500\t mse loss: 0.03324\t entropy1: 4.93663\t entropy2: 4.36856\t Huffman bitrate is 4.996\n",
      "iteration 5600\t mse loss: 0.02726\t entropy1: 4.78732\t entropy2: 4.15751\t Huffman bitrate is 4.867\n",
      "iteration 5700\t mse loss: 0.04712\t entropy1: 5.10494\t entropy2: 4.62882\t Huffman bitrate is 5.145\n",
      "iteration 5800\t mse loss: 0.04372\t entropy1: 5.07040\t entropy2: 4.57466\t Huffman bitrate is 5.109\n",
      "iteration 5900\t mse loss: 0.04505\t entropy1: 5.09539\t entropy2: 4.60663\t Huffman bitrate is 5.136\n",
      "iteration 6000\t mse loss: 0.02147\t entropy1: 4.76227\t entropy2: 4.12001\t Huffman bitrate is 4.887\n",
      "iteration 6100\t mse loss: 0.04050\t entropy1: 5.02161\t entropy2: 4.51267\t Huffman bitrate is 5.063\n",
      "iteration 6200\t mse loss: 0.02278\t entropy1: 4.73266\t entropy2: 4.08881\t Huffman bitrate is 4.838\n",
      "iteration 6300\t mse loss: 0.03170\t entropy1: 4.87410\t entropy2: 4.31107\t Huffman bitrate is 4.940\n",
      "iteration 6400\t mse loss: 0.04793\t entropy1: 5.07696\t entropy2: 4.61787\t Huffman bitrate is 5.114\n",
      "iteration 6500\t mse loss: 0.07126\t entropy1: 5.38376\t entropy2: 5.02144\t Huffman bitrate is 5.429\n",
      "iteration 6600\t mse loss: 0.04552\t entropy1: 4.99728\t entropy2: 4.50323\t Huffman bitrate is 5.033\n",
      "iteration 6700\t mse loss: 0.02185\t entropy1: 4.69837\t entropy2: 4.06445\t Huffman bitrate is 4.807\n",
      "iteration 6800\t mse loss: 0.02332\t entropy1: 4.73975\t entropy2: 4.12693\t Huffman bitrate is 4.841\n",
      "iteration 6900\t mse loss: 0.01958\t entropy1: 4.68041\t entropy2: 4.04119\t Huffman bitrate is 4.823\n",
      "iteration 7000\t mse loss: 0.03653\t entropy1: 4.87202\t entropy2: 4.33514\t Huffman bitrate is 4.924\n",
      "iteration 7100\t mse loss: 0.02543\t entropy1: 4.73865\t entropy2: 4.14744\t Huffman bitrate is 4.824\n",
      "iteration 7200\t mse loss: 0.02267\t entropy1: 4.70634\t entropy2: 4.09674\t Huffman bitrate is 4.809\n",
      "iteration 7300\t mse loss: 0.04257\t entropy1: 4.95291\t entropy2: 4.45293\t Huffman bitrate is 4.993\n",
      "iteration 7400\t mse loss: 0.04000\t entropy1: 4.91524\t entropy2: 4.40035\t Huffman bitrate is 4.958\n",
      "iteration 7500\t mse loss: 0.01952\t entropy1: 4.65407\t entropy2: 4.01845\t Huffman bitrate is 4.787\n",
      "iteration 7600\t mse loss: 0.02817\t entropy1: 4.76874\t entropy2: 4.19771\t Huffman bitrate is 4.847\n",
      "iteration 7700\t mse loss: 0.02122\t entropy1: 4.70066\t entropy2: 4.07868\t Huffman bitrate is 4.817\n",
      "iteration 7800\t mse loss: 0.02326\t entropy1: 4.74581\t entropy2: 4.13668\t Huffman bitrate is 4.846\n",
      "iteration 7900\t mse loss: 0.04591\t entropy1: 4.99065\t entropy2: 4.49717\t Huffman bitrate is 5.026\n",
      "iteration 8000\t mse loss: 0.02105\t entropy1: 4.72902\t entropy2: 4.10063\t Huffman bitrate is 4.852\n",
      "iteration 8100\t mse loss: 0.03839\t entropy1: 4.95379\t entropy2: 4.45260\t Huffman bitrate is 4.998\n",
      "iteration 8200\t mse loss: 0.04370\t entropy1: 5.02440\t entropy2: 4.55384\t Huffman bitrate is 5.064\n",
      "iteration 8300\t mse loss: 0.02554\t entropy1: 4.77265\t entropy2: 4.18272\t Huffman bitrate is 4.860\n",
      "iteration 8400\t mse loss: 0.02517\t entropy1: 4.77373\t entropy2: 4.18119\t Huffman bitrate is 4.863\n",
      "iteration 8500\t mse loss: 0.01955\t entropy1: 4.68434\t entropy2: 4.05213\t Huffman bitrate is 4.824\n",
      "iteration 8600\t mse loss: 0.01961\t entropy1: 4.67886\t entropy2: 4.04790\t Huffman bitrate is 4.820\n",
      "iteration 8700\t mse loss: 0.02217\t entropy1: 4.67835\t entropy2: 4.05313\t Huffman bitrate is 4.780\n",
      "iteration 8800\t mse loss: 0.02574\t entropy1: 4.75907\t entropy2: 4.17797\t Huffman bitrate is 4.846\n",
      "iteration 8900\t mse loss: 0.02536\t entropy1: 4.73330\t entropy2: 4.15522\t Huffman bitrate is 4.820\n",
      "iteration 9000\t mse loss: 0.05764\t entropy1: 5.18010\t entropy2: 4.78783\t Huffman bitrate is 5.219\n",
      "iteration 9100\t mse loss: 0.03647\t entropy1: 4.83757\t entropy2: 4.31007\t Huffman bitrate is 4.890\n",
      "iteration 9200\t mse loss: 0.02381\t entropy1: 4.67848\t entropy2: 4.07547\t Huffman bitrate is 4.767\n",
      "iteration 9300\t mse loss: 0.01883\t entropy1: 4.63414\t entropy2: 4.01044\t Huffman bitrate is 4.776\n",
      "iteration 9400\t mse loss: 0.01954\t entropy1: 4.64263\t entropy2: 4.02837\t Huffman bitrate is 4.771\n",
      "iteration 9500\t mse loss: 0.02862\t entropy1: 4.75215\t entropy2: 4.20276\t Huffman bitrate is 4.829\n",
      "iteration 9600\t mse loss: 0.01864\t entropy1: 4.61995\t entropy2: 3.99930\t Huffman bitrate is 4.761\n",
      "iteration 9700\t mse loss: 0.02507\t entropy1: 4.69804\t entropy2: 4.12889\t Huffman bitrate is 4.783\n",
      "iteration 9800\t mse loss: 0.02218\t entropy1: 4.64146\t entropy2: 4.03740\t Huffman bitrate is 4.740\n",
      "iteration 9900\t mse loss: 0.02192\t entropy1: 4.63940\t entropy2: 4.03359\t Huffman bitrate is 4.738\n",
      "iteration 10000\t mse loss: 0.02207\t entropy1: 4.64415\t entropy2: 4.03820\t Huffman bitrate is 4.743\n",
      "iteration 10100\t mse loss: 0.01885\t entropy1: 4.61394\t entropy2: 3.99587\t Huffman bitrate is 4.749\n",
      "iteration 10200\t mse loss: 0.01888\t entropy1: 4.62470\t entropy2: 4.00486\t Huffman bitrate is 4.764\n",
      "iteration 10300\t mse loss: 0.02357\t entropy1: 4.71205\t entropy2: 4.13233\t Huffman bitrate is 4.803\n",
      "iteration 10400\t mse loss: 0.04611\t entropy1: 4.95580\t entropy2: 4.46952\t Huffman bitrate is 4.992\n",
      "iteration 10500\t mse loss: 0.03762\t entropy1: 4.83412\t entropy2: 4.31085\t Huffman bitrate is 4.884\n",
      "iteration 10600\t mse loss: 0.03210\t entropy1: 4.84510\t entropy2: 4.32097\t Huffman bitrate is 4.907\n",
      "iteration 10700\t mse loss: 0.03204\t entropy1: 4.83567\t entropy2: 4.31793\t Huffman bitrate is 4.897\n",
      "iteration 10800\t mse loss: 0.02030\t entropy1: 4.67400\t entropy2: 4.07249\t Huffman bitrate is 4.793\n",
      "iteration 10900\t mse loss: 0.01900\t entropy1: 4.64944\t entropy2: 4.03910\t Huffman bitrate is 4.791\n",
      "iteration 11000\t mse loss: 0.02167\t entropy1: 4.68640\t entropy2: 4.09848\t Huffman bitrate is 4.791\n",
      "iteration 11100\t mse loss: 0.03211\t entropy1: 4.83922\t entropy2: 4.31934\t Huffman bitrate is 4.900\n",
      "iteration 11200\t mse loss: 0.02892\t entropy1: 4.79568\t entropy2: 4.25090\t Huffman bitrate is 4.870\n",
      "iteration 11300\t mse loss: 0.02436\t entropy1: 4.70985\t entropy2: 4.13787\t Huffman bitrate is 4.800\n",
      "iteration 11400\t mse loss: 0.04495\t entropy1: 4.99809\t entropy2: 4.55851\t Huffman bitrate is 5.036\n",
      "iteration 11500\t mse loss: 0.04534\t entropy1: 4.98633\t entropy2: 4.55291\t Huffman bitrate is 5.023\n",
      "iteration 11600\t mse loss: 0.05114\t entropy1: 5.01285\t entropy2: 4.57837\t Huffman bitrate is 5.046\n",
      "iteration 11700\t mse loss: 0.02052\t entropy1: 4.61800\t entropy2: 4.01383\t Huffman bitrate is 4.731\n",
      "iteration 11800\t mse loss: 0.01863\t entropy1: 4.60036\t entropy2: 3.99255\t Huffman bitrate is 4.735\n",
      "iteration 11900\t mse loss: 0.01832\t entropy1: 4.60036\t entropy2: 3.99205\t Huffman bitrate is 4.743\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ad249a5c0d2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#print(err_value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mtest_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-ad249a5c0d2d>\u001b[0m in \u001b[0;36mtest_classification\u001b[0;34m(model_function, learning_rate)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0macc_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0my_pred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0merr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlabel_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;31m#print(err_value.reshape([-1]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mentropy1_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##WORKING CODE WITH MSE\n",
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of bad initialization\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            #loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #SHANNON ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        ids=[i for i in range(100)]\n",
    "        for iter_i in range(30001):\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            ids=[(ids[0]+100+i)%label.shape[0] for i in range(100)]\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "                #print(err_value.reshape([-1]))\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "        err_value =  np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-label_n)*255)\n",
    "        #print(err_value)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4-b: MLP with Entropy Loss\n",
    "\n",
    "**Lookup:** Parzen Windowing for entropy estimation\n",
    "\n",
    "Renyi's Entropy with degree 2\n",
    "\n",
    "Details will be added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size is 128, batch overalpping is 0.70\n",
      "[[-0.01512186]\n",
      " [-0.01370698]\n",
      " [-0.01327906]\n",
      " [-0.01371549]\n",
      " [-0.01202339]\n",
      " [-0.01400765]\n",
      " [-0.01419603]\n",
      " [-0.01414287]\n",
      " [-0.01463538]\n",
      " [-0.01376984]\n",
      " [-0.01365946]\n",
      " [-0.01430218]\n",
      " [-0.01341727]\n",
      " [-0.01366693]\n",
      " [-0.01208443]\n",
      " [-0.0140257 ]\n",
      " [-0.01298509]\n",
      " [-0.01462172]\n",
      " [-0.01397931]\n",
      " [-0.01296171]\n",
      " [-0.01351203]\n",
      " [-0.01320543]\n",
      " [-0.01329373]\n",
      " [-0.01359024]\n",
      " [-0.01298061]\n",
      " [-0.01418507]\n",
      " [-0.01246071]\n",
      " [-0.01483695]\n",
      " [-0.01213941]\n",
      " [-0.0150045 ]\n",
      " [-0.01281254]\n",
      " [-0.01406704]\n",
      " [-0.01344297]\n",
      " [-0.01361258]\n",
      " [-0.01310156]\n",
      " [-0.01420983]\n",
      " [-0.01395277]\n",
      " [-0.01379903]\n",
      " [-0.01389919]\n",
      " [-0.01323907]\n",
      " [-0.01279315]\n",
      " [-0.01447896]\n",
      " [-0.01310856]\n",
      " [-0.01534556]\n",
      " [-0.01389434]\n",
      " [-0.01451541]\n",
      " [-0.01211028]\n",
      " [-0.01423732]\n",
      " [-0.01287516]\n",
      " [-0.01270284]\n",
      " [-0.01490566]\n",
      " [-0.01316111]\n",
      " [-0.01385357]\n",
      " [-0.01111775]\n",
      " [-0.01214324]\n",
      " [-0.01177426]\n",
      " [-0.01279517]\n",
      " [-0.01437956]\n",
      " [-0.01303306]\n",
      " [-0.01353825]\n",
      " [-0.01192316]\n",
      " [-0.01192942]\n",
      " [-0.01221862]\n",
      " [-0.00894901]\n",
      " [-0.01037329]\n",
      " [-0.00748162]\n",
      " [-0.0078638 ]\n",
      " [-0.00649894]\n",
      " [-0.00843315]\n",
      " [-0.00895192]\n",
      " [-0.00766748]\n",
      " [-0.00920763]\n",
      " [-0.00874111]\n",
      " [-0.00881735]\n",
      " [-0.00827613]\n",
      " [-0.00848756]\n",
      " [-0.0083539 ]\n",
      " [-0.00930187]\n",
      " [-0.00983796]\n",
      " [-0.00981806]\n",
      " [-0.0093696 ]\n",
      " [-0.00893475]\n",
      " [-0.00883644]\n",
      " [-0.00878871]\n",
      " [-0.00948405]\n",
      " [-0.01006852]\n",
      " [-0.00927479]\n",
      " [-0.00952631]\n",
      " [-0.00891823]\n",
      " [-0.00882627]\n",
      " [-0.00904458]\n",
      " [-0.0095607 ]\n",
      " [-0.00855056]\n",
      " [-0.00929531]\n",
      " [-0.0091628 ]\n",
      " [-0.01005168]\n",
      " [-0.01084619]\n",
      " [-0.00930957]\n",
      " [-0.01060049]\n",
      " [-0.00871492]\n",
      " [-0.00916332]\n",
      " [-0.00875792]\n",
      " [-0.00905414]\n",
      " [-0.01013198]\n",
      " [-0.00782638]\n",
      " [-0.01041257]\n",
      " [-0.009103  ]\n",
      " [-0.00974715]\n",
      " [-0.01064338]\n",
      " [-0.01018309]\n",
      " [-0.00928132]\n",
      " [-0.01001787]\n",
      " [-0.01003237]\n",
      " [-0.01079116]\n",
      " [-0.01068493]\n",
      " [-0.01049073]\n",
      " [-0.00986798]\n",
      " [-0.00994904]\n",
      " [-0.01134914]\n",
      " [-0.01115397]\n",
      " [-0.01255642]\n",
      " [-0.00974623]\n",
      " [-0.01054468]\n",
      " [-0.00984279]\n",
      " [-0.01050778]\n",
      " [-0.0106572 ]\n",
      " [-0.01190096]\n",
      " [-0.01088849]]\n",
      "iteration 0\t mse loss: 4.57835\t entropy1: 5.48752\t entropy2: 5.27208\t Huffman bitrate is 5.516\n",
      "Huffman bitrate is 7.484\n",
      "[[-0.00903685]\n",
      " [-0.00833798]\n",
      " [-0.00868321]\n",
      " [-0.00834492]\n",
      " [-0.00674532]\n",
      " [-0.00810329]\n",
      " [-0.00800076]\n",
      " [-0.00747558]\n",
      " [-0.00884653]\n",
      " [-0.00831966]\n",
      " [-0.00816032]\n",
      " [-0.00696521]\n",
      " [-0.00746395]\n",
      " [-0.00729694]\n",
      " [-0.00777045]\n",
      " [-0.00835802]\n",
      " [-0.00735275]\n",
      " [-0.00763284]\n",
      " [-0.00645011]\n",
      " [-0.0072891 ]\n",
      " [-0.00667455]\n",
      " [-0.00736079]\n",
      " [-0.00764999]\n",
      " [-0.00760666]\n",
      " [-0.00780044]\n",
      " [-0.00844089]\n",
      " [-0.00706954]\n",
      " [-0.00562958]\n",
      " [-0.00732379]\n",
      " [-0.00691608]\n",
      " [-0.00802967]\n",
      " [-0.00680626]\n",
      " [-0.00841657]\n",
      " [-0.00721737]\n",
      " [-0.00897544]\n",
      " [-0.00804434]\n",
      " [-0.00418293]\n",
      " [-0.00560721]\n",
      " [ 0.        ]\n",
      " [-0.00532885]\n",
      " [-0.0083753 ]\n",
      " [-0.00731232]\n",
      " [-0.0081396 ]\n",
      " [-0.0088513 ]\n",
      " [-0.00855215]\n",
      " [-0.00958915]\n",
      " [-0.00993835]\n",
      " [-0.00856501]\n",
      " [-0.00930982]\n",
      " [-0.00853213]\n",
      " [-0.00928291]\n",
      " [-0.00938107]\n",
      " [-0.01028648]\n",
      " [-0.01007628]\n",
      " [-0.01001936]\n",
      " [-0.01077501]\n",
      " [-0.00914694]\n",
      " [-0.01070797]\n",
      " [-0.00964131]\n",
      " [-0.00877212]\n",
      " [-0.01026395]\n",
      " [-0.00991521]\n",
      " [-0.00945027]\n",
      " [-0.00974936]\n",
      " [-0.00893033]\n",
      " [-0.00873025]\n",
      " [-0.00904919]\n",
      " [-0.00941702]\n",
      " [-0.00904639]\n",
      " [-0.00944379]\n",
      " [-0.00959582]\n",
      " [-0.00869649]\n",
      " [-0.00840712]\n",
      " [-0.00990705]\n",
      " [-0.00877741]\n",
      " [-0.00950481]\n",
      " [-0.01004177]\n",
      " [-0.00879811]\n",
      " [-0.0098657 ]\n",
      " [-0.00993179]\n",
      " [-0.00927907]\n",
      " [-0.00944839]\n",
      " [-0.00938309]\n",
      " [-0.01024592]\n",
      " [-0.00932122]\n",
      " [-0.00965049]\n",
      " [-0.00925149]\n",
      " [-0.00939822]\n",
      " [-0.00907947]\n",
      " [-0.00995402]\n",
      " [-0.00924147]\n",
      " [-0.01022604]\n",
      " [-0.00921889]\n",
      " [-0.00972655]\n",
      " [-0.00869484]\n",
      " [-0.0097131 ]\n",
      " [-0.00909052]\n",
      " [-0.00923714]\n",
      " [-0.0092597 ]\n",
      " [-0.00976625]\n",
      " [-0.00841995]\n",
      " [-0.00936848]\n",
      " [-0.00903142]\n",
      " [-0.00874326]\n",
      " [-0.00996531]\n",
      " [-0.01006302]\n",
      " [-0.0096763 ]\n",
      " [-0.00982475]\n",
      " [-0.00973214]\n",
      " [-0.00848132]\n",
      " [-0.00914438]\n",
      " [-0.01054318]\n",
      " [-0.01210091]\n",
      " [-0.0130259 ]\n",
      " [-0.01255971]\n",
      " [-0.00986669]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.00628301]\n",
      " [-0.01081302]\n",
      " [-0.01179581]\n",
      " [-0.01240298]\n",
      " [-0.01285607]\n",
      " [-0.01272746]\n",
      " [-0.01288082]\n",
      " [-0.01240801]]\n",
      "iteration 100\t mse loss: 4.41657\t entropy1: 5.17101\t entropy2: 4.79543\t Huffman bitrate is 5.188\n",
      "Huffman bitrate is 7.484\n",
      "[[-0.00272057]\n",
      " [-0.00293679]\n",
      " [-0.00334871]\n",
      " [-0.00475392]\n",
      " [-0.00456625]\n",
      " [-0.00397576]\n",
      " [-0.00057053]\n",
      " [-0.00344347]\n",
      " [-0.0030845 ]\n",
      " [-0.00424892]\n",
      " [-0.00316185]\n",
      " [-0.00369479]\n",
      " [-0.00416784]\n",
      " [-0.00396787]\n",
      " [-0.00490955]\n",
      " [-0.00459995]\n",
      " [-0.00349875]\n",
      " [-0.00332782]\n",
      " [-0.00413488]\n",
      " [-0.00322511]\n",
      " [-0.00418606]\n",
      " [-0.00308519]\n",
      " [-0.00391278]\n",
      " [-0.00400548]\n",
      " [-0.00410557]\n",
      " [-0.00530038]\n",
      " [-0.00465454]\n",
      " [-0.00408479]\n",
      " [-0.00380574]\n",
      " [-0.00331   ]\n",
      " [-0.00378517]\n",
      " [-0.00380296]\n",
      " [-0.00425785]\n",
      " [-0.00449122]\n",
      " [-0.00336702]\n",
      " [-0.00420809]\n",
      " [-0.00384348]\n",
      " [-0.00391512]\n",
      " [-0.00387806]\n",
      " [-0.00400355]\n",
      " [-0.00350304]\n",
      " [-0.00436175]\n",
      " [-0.00433519]\n",
      " [-0.00380372]\n",
      " [-0.00377548]\n",
      " [-0.00385476]\n",
      " [-0.00435946]\n",
      " [-0.00410327]\n",
      " [-0.00508763]\n",
      " [-0.00512688]\n",
      " [-0.00480795]\n",
      " [-0.00441028]\n",
      " [-0.00441329]\n",
      " [-0.00440017]\n",
      " [-0.00428923]\n",
      " [-0.0049356 ]\n",
      " [-0.00527632]\n",
      " [-0.00393077]\n",
      " [-0.00418297]\n",
      " [-0.00469766]\n",
      " [-0.00546635]\n",
      " [-0.00497688]\n",
      " [-0.00555543]\n",
      " [-0.00479938]\n",
      " [-0.00452869]\n",
      " [-0.00391306]\n",
      " [-0.00451904]\n",
      " [-0.00509304]\n",
      " [-0.00379867]\n",
      " [-0.0055802 ]\n",
      " [-0.00471075]\n",
      " [-0.00522048]\n",
      " [-0.00491772]\n",
      " [-0.00474713]\n",
      " [-0.00389432]\n",
      " [-0.00600534]\n",
      " [-0.00383776]\n",
      " [-0.00512048]\n",
      " [-0.00439107]\n",
      " [-0.00458613]\n",
      " [-0.00488444]\n",
      " [-0.00490987]\n",
      " [-0.00363857]\n",
      " [-0.00494284]\n",
      " [-0.00434448]\n",
      " [-0.00539232]\n",
      " [-0.00456376]\n",
      " [-0.0049933 ]\n",
      " [-0.00397758]\n",
      " [-0.0048749 ]\n",
      " [-0.00423026]\n",
      " [-0.00399671]\n",
      " [-0.00508326]\n",
      " [-0.00492638]\n",
      " [-0.00476465]\n",
      " [-0.00480481]\n",
      " [-0.00434134]\n",
      " [-0.00447365]\n",
      " [-0.00394172]\n",
      " [-0.0055086 ]\n",
      " [-0.0051899 ]\n",
      " [-0.00518834]\n",
      " [-0.00499155]\n",
      " [-0.00438179]\n",
      " [-0.00481954]\n",
      " [-0.00393579]\n",
      " [-0.00516456]\n",
      " [-0.00375669]\n",
      " [-0.0045967 ]\n",
      " [-0.0052429 ]\n",
      " [-0.00489954]\n",
      " [-0.00521911]\n",
      " [-0.00455766]\n",
      " [-0.0058789 ]\n",
      " [-0.00436221]\n",
      " [-0.00507693]\n",
      " [-0.0049198 ]\n",
      " [-0.00539549]\n",
      " [-0.00471185]\n",
      " [-0.00499517]\n",
      " [-0.00565649]\n",
      " [-0.00524981]\n",
      " [-0.00530802]\n",
      " [-0.00482288]\n",
      " [-0.00425774]\n",
      " [-0.00514599]\n",
      " [-0.00507691]\n",
      " [-0.00570169]]\n",
      "iteration 200\t mse loss: 3.62123\t entropy1: 4.67197\t entropy2: 4.25853\t Huffman bitrate is 4.703\n",
      "Huffman bitrate is 7.474\n",
      "[[-0.00193695]\n",
      " [-0.00225584]\n",
      " [-0.00246911]\n",
      " [-0.00197885]\n",
      " [-0.0027786 ]\n",
      " [-0.00284691]\n",
      " [-0.00299037]\n",
      " [-0.00250642]\n",
      " [-0.00252529]\n",
      " [-0.0022928 ]\n",
      " [-0.00168322]\n",
      " [-0.00288671]\n",
      " [-0.00181043]\n",
      " [-0.00336859]\n",
      " [-0.00166098]\n",
      " [-0.00309827]\n",
      " [-0.00214514]\n",
      " [-0.0025515 ]\n",
      " [-0.00247164]\n",
      " [-0.00241144]\n",
      " [-0.00228258]\n",
      " [-0.00171332]\n",
      " [-0.00246294]\n",
      " [-0.00203993]\n",
      " [-0.00164229]\n",
      " [-0.00262532]\n",
      " [-0.0026961 ]\n",
      " [-0.00203802]\n",
      " [-0.00213657]\n",
      " [-0.00239734]\n",
      " [-0.00278625]\n",
      " [-0.00205814]\n",
      " [-0.00272459]\n",
      " [-0.00173073]\n",
      " [-0.0023319 ]\n",
      " [-0.00175004]\n",
      " [-0.00245815]\n",
      " [-0.00237756]\n",
      " [-0.00306354]\n",
      " [-0.00182391]\n",
      " [-0.00250357]\n",
      " [-0.00226125]\n",
      " [-0.00183902]\n",
      " [-0.00252405]\n",
      " [-0.00209747]\n",
      " [-0.00170007]\n",
      " [-0.00214968]\n",
      " [-0.00220469]\n",
      " [-0.00135611]\n",
      " [-0.00248574]\n",
      " [-0.00235477]\n",
      " [-0.00340467]\n",
      " [-0.00317984]\n",
      " [-0.00310211]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.0024072 ]\n",
      " [-0.00231209]\n",
      " [-0.00224233]\n",
      " [-0.00275336]\n",
      " [-0.00251875]\n",
      " [-0.00254765]\n",
      " [-0.00293553]\n",
      " [-0.00326617]\n",
      " [-0.003492  ]\n",
      " [-0.0025883 ]\n",
      " [-0.00147212]\n",
      " [-0.00139998]\n",
      " [-0.00320113]\n",
      " [-0.00372179]\n",
      " [-0.00435679]\n",
      " [-0.00701742]\n",
      " [-0.00591274]\n",
      " [-0.00614849]\n",
      " [-0.00544397]\n",
      " [-0.00314482]\n",
      " [-0.00248265]\n",
      " [-0.00177839]\n",
      " [-0.00144833]\n",
      " [-0.00165317]\n",
      " [-0.00215887]\n",
      " [-0.00112854]\n",
      " [-0.00152086]\n",
      " [-0.00210631]\n",
      " [-0.00138159]\n",
      " [-0.00159323]\n",
      " [-0.00154534]\n",
      " [-0.00151021]\n",
      " [-0.00193621]\n",
      " [-0.00135053]\n",
      " [-0.00147817]\n",
      " [-0.00182513]\n",
      " [-0.00116377]\n",
      " [-0.0022597 ]\n",
      " [-0.00172202]\n",
      " [-0.00239764]\n",
      " [-0.00272511]\n",
      " [-0.00184663]\n",
      " [-0.00185922]\n",
      " [-0.00133614]\n",
      " [-0.0017022 ]\n",
      " [-0.00213508]\n",
      " [-0.00071969]\n",
      " [-0.00208028]\n",
      " [-0.00106933]\n",
      " [-0.00201444]\n",
      " [-0.00236924]\n",
      " [-0.00148496]\n",
      " [-0.00183015]\n",
      " [-0.00226217]\n",
      " [-0.00239496]\n",
      " [-0.00235605]\n",
      " [-0.00163142]\n",
      " [-0.00135664]\n",
      " [-0.00182423]\n",
      " [-0.00135136]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.00117642]\n",
      " [-0.00027859]\n",
      " [-0.00388408]\n",
      " [-0.00578355]\n",
      " [-0.00594504]\n",
      " [-0.00741159]]\n",
      "iteration 300\t mse loss: 4.78152\t entropy1: 5.68059\t entropy2: 5.38529\t Huffman bitrate is 5.703\n",
      "Huffman bitrate is 7.472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-b325eb665407>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Huffman bitrate is %.3f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mhuffman_bpp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m \u001b[0mtest_classification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd_dev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-b325eb665407>\u001b[0m in \u001b[0;36mtest_classification\u001b[0;34m(model_function, learning_rate, batch_size, batch_overlap)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_value3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             '''\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# test trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#It was wotking before but know stucks at zero, dont know the reason\n",
    "\n",
    "#Normalize the vectors and labels\n",
    "#Sometimes does not work beacuse of wron initialization\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "\n",
    "\n",
    "def test_classification(model_function, learning_rate=0.1,batch_size=128,batch_overlap=.7):\n",
    "\n",
    "    n=batch_size\n",
    "    batch_inc = int(batch_size*(1-batch_overlap))\n",
    "    print('Batch size is %d, batch overalpping is %.2f'%(batch_size,batch_overlap))\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            #loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #RENYI ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            #loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #\"\"\"\n",
    "            \n",
    "            c=np.power(n,-.2).astype(np.float32)\n",
    "            err_=tf.transpose(tf.subtract(y_,y_logits))*255\n",
    "            x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "            x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "            h_=tf.reshape(tf.multiply(1.06,tf.multiply(x_std,c)),[-1,1])\n",
    "            #h_=tf.multiply(1.06,tf.multiply(x_std,c))\n",
    "\n",
    "\n",
    "            #prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "            #Calculate Entropy\n",
    "            prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "            prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "            #Calculate Entropy\n",
    "            sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "            for k in range(1,510):\n",
    "                #print(k)\n",
    "                prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "                prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "                #Calculate Entropy\n",
    "                sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "            renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "            loss=tf.reduce_mean(renyi_ent2)\n",
    "            #\"\"\"\n",
    "            \n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            #train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "            #                                    epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        \n",
    "        ids=[i for i in range(batch_size)]\n",
    "        for iter_i in range(100001):\n",
    "            #print(iter_i)\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            #print(ids)\n",
    "            ids=[(ids[0]+batch_inc+i)%label.shape[0] for i in range(batch_size)]\n",
    "            \n",
    "            tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "            #acc_value = sess.run(c, feed_dict=tf_feed_dict)\n",
    "            '''\n",
    "            print(c)\n",
    "            acc_value = sess.run(err_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(x_std, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(h_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "            acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "            y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "            err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "            \n",
    "            print(acc_value2)\n",
    "            print(acc_value3)\n",
    "            '''\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 100 == 0:\n",
    "                #tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "                \n",
    "                '''\n",
    "                acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "                acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "                print(acc_value)\n",
    "                print(acc_value2)\n",
    "                print(acc_value3)\n",
    "                print(err_value.reshape([-1]))\n",
    "                '''\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print(y_pred_val*255)\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "                err_value =  np.round((sess.run(y_pred, feed_dict={x_: fvec_n, y_: label_n})-label_n)*255)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('Huffman bitrate is %.3f'%huffman_bpp)\n",
    "                \n",
    "test_classification(lambda x: mlp(x, [16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_classification(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learn_rate=5e-3\n",
    "                    ,batch_size=128,batch_overlap=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size is 128, batch overalpping is 0.70\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#TEST PHASE. MAY NOT WORK\n",
    "#Look for modularity. You can use the old homework\n",
    "\n",
    "fvec_n=fvec/np.round(np.max(label))\n",
    "label_n = label/np.round(np.max(label))\n",
    "def build_entropy_network(model_function, learning_rate=0.1,batch_size=128,batch_overlap=.7):\n",
    "\n",
    "    n=batch_size\n",
    "    batch_inc = int(batch_size*(1-batch_overlap))\n",
    "    print('Batch size is %d, batch overalpping is %.2f'%(batch_size,batch_overlap))\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, fvec_n.shape[1]])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            #Basic MSE loss\n",
    "            #loss = tf.reduce_mean(tf.pow(tf.subtract(y_,y_logits), 2.0))\n",
    "            \n",
    "            #RENYI ENTROPY IS NOT DIFFERENTIABLE\n",
    "            #kernel = tf.contrib.distributions.Normal(mu=0., sigma=1.)\n",
    "            #loss = tf.contrib.bayesflow.entropy.entropy_shannon(p=kernel,z=tf.subtract(y_,y_logits))\n",
    "            #loss = tf.reduce_mean(tf.abs(tf.subtract(y_,y_logits)))\n",
    "            #\"\"\"\n",
    "            \n",
    "            c=np.power(n,-.2).astype(np.float32)\n",
    "            err_=tf.transpose(tf.subtract(y_,y_logits))*255\n",
    "            x_mean,x_std=tf.nn.moments(err_,axes=[1])\n",
    "            x_std=tf.sqrt(tf.multiply(x_std,float(n)/float(n-1)))\n",
    "            h_=tf.reshape(tf.multiply(1.06,tf.multiply(x_std,c)),[-1,1])\n",
    "            #h_=tf.multiply(1.06,tf.multiply(x_std,c))\n",
    "\n",
    "\n",
    "            #prob_mat1=tf.div(err_,tf.matmul(h_,np.ones([1,n]).astype(np.float32)))\n",
    "            #Calculate Entropy\n",
    "            prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "            prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "            #Calculate Entropy\n",
    "            sum_for_ent=tf.pow(prob_,2)\n",
    "\n",
    "\n",
    "            for k in range(1,510):\n",
    "                #print(k)\n",
    "                prob_mat = 1e-3+tf.exp(tf.multiply(-.5,tf.pow(tf.div((err_-k+255),tf.matmul(h_,np.ones([1,n]).astype(np.float32))),2)))\n",
    "                prob_ = tf.multiply((1/(np.sqrt(2*np.pi)*n*tf.transpose(h_))),tf.reduce_sum(prob_mat,axis=[1]))\n",
    "                #Calculate Entropy\n",
    "                sum_for_ent=sum_for_ent+tf.pow(prob_,2)\n",
    "            renyi_ent2=-tf.log(sum_for_ent)#/tf.log(2.)\n",
    "            loss=tf.reduce_mean(renyi_ent2)\n",
    "            #\"\"\"\n",
    "            \n",
    "            #train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate=5e-3,beta1=0.3,beta2=0.999, \n",
    "                                                epsilon=1e-08,use_locking=False).minimize(loss)\n",
    "           \n",
    "            y_pred = y_logits\n",
    "            correct_prediction = tf.equal(y_pred, y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return g\n",
    "\n",
    "            \n",
    "def test_classification(model_function, g,batch_size=128,batch_overlap=.7):  \n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train\n",
    "        #print(label.shape[0])\n",
    "        \n",
    "        ids=[i for i in range(batch_size)]\n",
    "        for iter_i in range(100001):\n",
    "            #print(iter_i)\n",
    "            #print(label.shape[0])\n",
    "            #print(2*my_range)\n",
    "            batch_xs = fvec_n[ids,:] \n",
    "            batch_ys = label_n[ids]\n",
    "            #print(ids)\n",
    "            ids=[(ids[0]+batch_inc+i)%label.shape[0] for i in range(batch_size)]\n",
    "            \n",
    "            tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "            #acc_value = sess.run(c, feed_dict=tf_feed_dict)\n",
    "            '''\n",
    "            print(c)\n",
    "            acc_value = sess.run(err_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(x_std, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value = sess.run(h_, feed_dict=tf_feed_dict)\n",
    "            print(acc_value)\n",
    "            acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "            acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "            y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "            err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "            \n",
    "            print(acc_value2)\n",
    "            print(acc_value3)\n",
    "            '''\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 100 == 0:\n",
    "                #tf_feed_dict = {x_: fvec_n, y_: label_n}\n",
    "                tf_feed_dict = {x_: batch_xs, y_: batch_ys}\n",
    "                acc_value = sess.run(loss, feed_dict=tf_feed_dict)\n",
    "                y_pred_val = sess.run(y_pred, feed_dict=tf_feed_dict)\n",
    "                err_value = np.round((sess.run(y_pred, feed_dict=tf_feed_dict)-batch_ys)*255)\n",
    "                \n",
    "                '''\n",
    "                acc_value2 = sess.run(y_logits, feed_dict=tf_feed_dict)\n",
    "                acc_value3 = sess.run(prob_mat, feed_dict=tf_feed_dict)\n",
    "                print(acc_value)\n",
    "                print(acc_value2)\n",
    "                print(acc_value3)\n",
    "                print(err_value.reshape([-1]))\n",
    "                '''\n",
    "                entropy1_err = entropy(err_value.reshape([-1]),1)\n",
    "                entropy2_err = entropy(err_value.reshape([-1]),2)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                #print(y_pred_val*255)\n",
    "                print('iteration %d\\t mse loss: %.5f\\t entropy1: %.5f\\t entropy2: %.5f\\t Huffman bitrate is %.3f'%\n",
    "                      (iter_i, acc_value, entropy1_err,entropy2_err,huffman_bpp))\n",
    "                err_value =  np.round((sess.run(y_pred, feed_dict={x_: fvec_n, y_: label_n})-label_n)*255)\n",
    "                huffman_err = huffman(err_value.reshape([-1]).astype(str))\n",
    "                huffman_bpp = float(len(huffman_err[1])/float(len(err_value)))\n",
    "                print('Huffman bitrate is %.3f'%huffman_bpp)\n",
    "                \n",
    "g=build_entropy_network(lambda x: mlp(x, [32,16,8,4,2,1], activation_fn=tf.nn.relu,std_dev=1e-1), learning_rate=0.01\n",
    "                    ,batch_size=128,batch_overlap=.7)\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
